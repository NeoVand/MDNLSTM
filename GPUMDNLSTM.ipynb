{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.load('LDS_Toy_Data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNLSTM(nn.Module):\n",
    "    def __init__(self, d_obs, d_lat=2, n_gaussians=2, n_layers=1):\n",
    "        super(MDNLSTM, self).__init__()\n",
    "\n",
    "        self.d_obs = d_obs\n",
    "        self.d_lat = d_lat\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(d_obs, d_lat, n_layers, batch_first=True)\n",
    "        self.fcPi = nn.Linear(d_lat, n_gaussians*d_obs)\n",
    "        self.fcMu = nn.Linear(d_lat, n_gaussians*d_obs)\n",
    "        self.fcSigma = nn.Linear(d_lat, n_gaussians*d_obs)\n",
    "        \n",
    "    def get_mixture_coef(self, y):\n",
    "        time_steps = y.size(1)\n",
    "        pi, mu, sigma = self.fcPi(y), self.fcMu(y), self.fcSigma(y)\n",
    "        \n",
    "        pi = pi.view(-1, time_steps, self.n_gaussians, self.d_obs)\n",
    "        mu = mu.view(-1, time_steps, self.n_gaussians, self.d_obs)\n",
    "        sigma = sigma.view(-1, time_steps, self.n_gaussians, self.d_obs)\n",
    "        \n",
    "        pi = F.softmax(pi, 2)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return pi, mu, sigma\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        y, (h, c) = self.lstm(x, h)\n",
    "        #print(h)\n",
    "        pi, mu, sigma = self.get_mixture_coef(y)\n",
    "        return (pi, mu, sigma), (h, c)\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        return (torch.zeros(self.n_layers, bsz, self.d_lat).to(device),\n",
    "                torch.zeros(self.n_layers, bsz, self.d_lat).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu, sigma):\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    loss = torch.exp(m.log_prob(y))\n",
    "    loss = torch.sum(loss * pi, dim=2)\n",
    "    loss = -torch.log(loss)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y, pi, mu, sigma):\n",
    "    y = y.unsqueeze(2)\n",
    "    return mdn_loss_fn(y, pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOBS = 10\n",
    "DLAT = 2\n",
    "INSTS = 100\n",
    "seqlen = 30\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "mdnlstm = MDNLSTM(DOBS, DLAT).to(device)\n",
    "optimizer = torch.optim.Adam(mdnlstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 300, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z =  torch.from_numpy(ts[:INSTS,:,:]).float().to(device)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Loss: nan\n",
      "Epoch [100/200], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# hiddens=[]\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    hidden = mdnlstm.init_hidden(100)\n",
    "    \n",
    "    for i in range(0, z.size(1) - seqlen, seqlen):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = z[:, i:i+seqlen, :]\n",
    "        targets = z[:, (i+1):(i+1)+seqlen, :]\n",
    "        \n",
    "        hidden = detach(hidden)\n",
    "#         hiddens.append(hidden)\n",
    "        (pi, mu, sigma), hidden = mdnlstm(inputs, hidden)\n",
    "        loss = criterion(targets, pi, mu, sigma)\n",
    "        mdnlstm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch, epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
